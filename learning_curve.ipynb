{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_zHfyv5avXr"
   },
   "source": [
    "# 1) Methodology and description\n",
    "* In Supervised Learning, we devised the hypothesis (f) which describes perfectly the relationship between features and target class. \n",
    "* We are very likely to get a different hypothesis (f) if we use different training set. We define variance as the amount by which (f) varies when using different training sets. Additionally, we use different classifiers to estimate the true (f). As we simply our approach and makes assumptions to estimate (f), we give more bias to our hypothesis.\n",
    "* Mathematically, we need low bias and low variance to minimise our model's error. A low-biased model fits training data very well but performs poor on test data. We want a low variance to avoid building a complex model. Hence we need to accept the trade-off between bias and variance.\n",
    "\n",
    "### Learning Curve\n",
    "\n",
    "* We take one single data pair from the training set and use it to estimate a model. Then we measure the model's error on the validation set and on that single training instance.\n",
    "* We then repeat the process with more amount of training data until we use our entire training set. Initially, the error on the training instance will be 0, since it's easy to perfectly fit a single data instances. Consequently, the error on the validation set will be very large since the model is built around a single instance.\n",
    "* As we increase the training set size, the model fits more perfectly the test data and fits training data less.\n",
    "* We can plot the learning curve of training data and test data to better understand the nature of a model.\n",
    "\n",
    "### Preparation steps\n",
    "\n",
    "* We used pandas' read_csv method to read our dataset 'autoimmune.csv'. Columns are separated by commas and rows are separated by newlines. Each column describes one individual patient. Column Autoimmune_Disease is our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bLsk1U5oavX5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ds = pd.read_csv('data/autoimmune.txt', delimiter=\"\\t\",header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lwZBltm2avYi"
   },
   "source": [
    "* Apply transpose function to map attributes as columns and patient as rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VtpX8jDKavY0"
   },
   "outputs": [],
   "source": [
    "ds=ds.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BCYwE3x-avZ_"
   },
   "source": [
    "* Assingning names to the columns as per the data set\n",
    "* Re-numbering data set index from 1 (using numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NmjtuGrNavaG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ds.columns=['Age','Blood_Pressure','BMI','Plasma_level','Autoimmune_Disease','Adverse_events','Drug_in_serum','Liver_function','Activity_test','Secondary_test']\n",
    "ds.index = np.arange(1, len(ds) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dx4hwovavaW"
   },
   "source": [
    "* Mapping data set into dependent attributes and class label attribute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3MO_sj9iavaa"
   },
   "outputs": [],
   "source": [
    "X = ds.drop('Autoimmune_Disease',axis=1)\n",
    "y = ds['Autoimmune_Disease']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DI-n_YFEavbb"
   },
   "source": [
    "#### Assumptions \n",
    "* There are no outliers or missing data.\n",
    "* Data presented is already scaled or don't required scaling.\n",
    "    \n",
    "#### Training set size\n",
    "* The minimum data size is 1. Our training set has 376 instances, so the maximum value is 376.\n",
    "* We kept aside 76 data for validation and plot curve with remaining 300 data.\n",
    "* here, we use six sizes:\n",
    "    * [1,50,150,200,250,300]\n",
    "    * For each of the size above, a new model will be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes=[1,50,150,200,250,300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Cross Validation\n",
    "* We will be using N Cross Validation to train our model for each training data size given above. In our case, N=10.\n",
    "\n",
    "#### Function: learning_curve()\n",
    "* We are using learning_curve() funtion from scikit-learn with following paramters.\n",
    "    * estimator: our classifier to fit and train model\n",
    "        * DecisionTreeClassifier(criterion='entropy',random_state = 100,\n",
    " max_depth=3)\n",
    "        * GaussianNB\n",
    "    * X: feature variables from the train data.\n",
    "    * y: target variable from the train data.\n",
    "    * cv: Cross Validation generator. In our case, we have used 10 fold Cross validation with (n_splits=10, shuffle=False, random_state=None)\n",
    "    * train_sizes: size of training data set defined above.\n",
    "\n",
    "#### Function: plot_learning_curve\n",
    "* It takes mean of train scores and test scores per cross validation. It uses these values to construct a learning curve.\n",
    "* Plot training set size on x-axis and error score on y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(title,train_scores,test_scores):\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    \n",
    "    #plot graph program code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Plotting Graph\n",
    "Learning curve for 6 training set sizes using 10-fold cross-validation.\n",
    "\n",
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    " DecisionTreeClassifier(criterion='entropy',random_state = 100,\n",
    " max_depth=3), X,y,train_sizes = train_sizes, cv = 10)\n",
    "\n",
    "plot_learning_curve(\"Learning curve for a Decision Tree Classifier\",\n",
    "                    train_scores,test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/decision.png\" style=\"width: 400px;float:left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    " GaussianNB(), X,y,train_sizes = train_sizes, cv = 10)\n",
    "\n",
    "plot_learning_curve(\"Learning curve for a Naive Bayes Classifier\",\n",
    "                    train_scores,test_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/nb.png\" style=\"width: 400px;float:left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Observation\n",
    "\n",
    "* (1) **[size=1]** When the training set size is 1, the training score is 1. This is normal behaviour since the model has no problem fitting perfectly a single data point. At this point, the validation score is 0. Since it's extremely unlikely that a model trained on a single data point can generalize 76 instances accurately to test data it hasn't seen in training.\n",
    "<br><br>\n",
    "* (2) **[size=50]** When the training set size increases to 50, the validation score rises sharply. Likewise, the training score decreases. The model performs much better now on the validation set because it is trained on more data (size=50) and so the estimation more correct now.\n",
    "\n",
    "    As seen from the learning curve,\n",
    "    * (Decision Tree Classifier)\n",
    "       * Validation score increases from (0.31374269 to 0.75599415)\n",
    "       * Training score decreases from (1 to 0.844)\n",
    "    * (Naive Bayes Classifier)\n",
    "       * Validation score increases from (0.31374269 to  0.73099415)\n",
    "       * Training score decrease from (1 to 0.81)\n",
    "<br><br>\n",
    "* (3) **[size=150]** When the training set size increases to 150, the validation score increases slightly. The training score decreases with the small amount. The model has now become more efficient on the validation set because it is trained on more data (size=150). The model is estimating accurately with more training data.\n",
    "\n",
    "    As seen from the learning curve,\n",
    "    * (Decision Tree Classifier)\n",
    "       * Validation score increases from (0.75599415 to 0.77383041)\n",
    "       * Training score decreases from (0.844 to 0.84333333)\n",
    "    * (Naive Bayes Classifier)\n",
    "       * Validation score increases from (0.73099415 to 0.76023392)\n",
    "       * Training score decreases from (0.81 to 0.78666667)\n",
    "<br><br>\n",
    "* (4) **[size=200]** \n",
    "<br>(Decision Tree Classifier)<br>\n",
    "When the training set size increases to 200, both the validation score and training score have come down. But our model has acquired the task more efficiently and correctly.\n",
    "   * Validation score increases from (0.77383041 to 0.72631579)\n",
    "   * Training score decreases from (0.84333333 to 0.8165)\n",
    "   \n",
    "* (Naive Bayes Classifier)<br>\n",
    "From 200 training data onwards, the validation and training score got almost near to each other. Consequently, the model performs much better now on the validation set because it's estimated with more data.\n",
    "   * Validation score jumps from (0.76023392  0.75789474)\n",
    "   * Training scores decrease from (0.78666667  0.777)\n",
    "<br><br>\n",
    "* (5) **[size=250]** \n",
    "<br>(Decision Tree Classifier)<br>\n",
    "When the training set size increases to 250, the validation score and training score have come closer. The point tells us that both the score will remain same no matter how much training data we supplied.\n",
    "   * Validation score increases from (0.72631579 to 0.76345029)\n",
    "   * Training score decreases from (0.8165 to 0.8092)\n",
    "   \n",
    "* (Naive Bayes Classifier)<br>\n",
    "From 250 training data onwards, the validation and training score stays roughly the same. We have an achieved an equilibrium between bias and variance.\n",
    "   * Validation score increases from (0.75789474 to 0.75526316)\n",
    "   * Training scores decrease from (0.777 to 0.7832)\n",
    "<br><br>\n",
    "* (6) **[size=300]** At 300 training data size for Naive Bayes, the validation and training scores converges to the same point. Adding more training data points won't lead to significantly better models. Switching to a different algorithm may produce different results and can adapt the data more perfectly. For Decision Tree Classifier, the validation score and training score could converge if more training instances are addded.\n",
    "  \n",
    "    As seen from the learning curve,\n",
    "    * (Decision Tree Classifier)\n",
    "       * Validation score increases from (0.76345029 to 0.76345029)\n",
    "       * Training score decreases from (0.8092 to 0.81366667)\n",
    "    * (Naive Bayes Classifier)\n",
    "       * Validation score increases from (0.75526316  0.76315789)\n",
    "       * Training scores decreases from (0.7832 to 0.78266667)\n",
    "\n",
    "# 4) Conclusion\n",
    "We can diagnose the above model using learning curve as follows:\n",
    "* The gap between curves becomes more and more narrow as we increase the training data size. Generally, the more narrow the gap, the lower the variance.\n",
    "* In our case, as we increased the training set sizes, the differences between the training and validation score narrowed down and so the gap between the two learning curves.\n",
    "* High gap indicates high training score and also high variance. In our case, the training score converges to the state of no change at around 250. Hence, the learning corroborated that we have a low variance model.\n",
    "* From the graph, we can deduce that training score has reduced with the increase in training set size. It means that our model has not fit well with the training data. This reflects a high bias model. But that is a trade-off that we have to weigh to get a low variance model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itIE8QTfavhw"
   },
   "source": [
    "# 5) References\n",
    "* Scikit-learn documentation. http://scikit-learn.org Accessed 1 Nov, 2018 \n",
    "* Pandas Library. http://pandas.pydata.org/pandas-docs/stable/ Accessed 1 Nov, 2018\n",
    "* DataQuest: https://www.dataquest.io/blog/learning-curves-machine-learning/ Accessed 1 Nov, 2018\n",
    "* Chapter 5, Page 137: Mitchell, Tom, 1997. Machine Learning. International ed. McGraw-Hill.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
